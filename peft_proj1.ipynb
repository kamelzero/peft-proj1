{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: \n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5737397c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f551c63a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_398/961986339.py:21: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_balanced = df.groupby('safety_label').apply(lambda x: x.sample(min_samples)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "\n",
    "# this dataset has text labeleed according to its social appropriateness. \n",
    "dataset = load_dataset('allenai/prosocial-dialog')\n",
    "combined_dataset = concatenate_datasets([dataset['train'], dataset['validation'], dataset['test']])\n",
    "filtered_dataset = combined_dataset.filter(lambda example: example['safety_label'] in ['__casual__', '__needs_intervention__'])\n",
    "df = filtered_dataset.to_pandas()\n",
    "\n",
    "min_samples = min(df['safety_label'].value_counts()['__casual__'], df['safety_label'].value_counts()['__needs_intervention__'])\n",
    "df_balanced = df.groupby('safety_label').apply(lambda x: x.sample(min_samples)).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81c1886",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "safety_label\n",
       "__casual__                21646\n",
       "__needs_intervention__    21646\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_balanced.safety_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce1886a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/student/.local/lib/python3.10/site-packages (1.5.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/student/.local/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.2)\n"
     ]
    }
   ],
   "source": [
    "! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n",
    "train_df, temp_df = train_test_split(df_balanced, test_size=0.4, stratify=df_balanced['safety_label'])\n",
    "validation_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['safety_label'])\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441e9baebe6e4895807fe2ad54fbbb39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25975 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4348f60d72d54bff80cc976edc102be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8658 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01c3ba74fb74d3eadc20cdbf2f9e1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8659 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    tokenized_inputs = tokenizer(examples['context'], truncation=True, padding='max_length', max_length=512)\n",
    "    tokenized_inputs['labels'] = [0 if label == '__casual__' else 1 for label in examples['safety_label']]\n",
    "    return tokenized_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "validation_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora.py:475: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, GPT2ForSequenceClassification\n",
    "from sklearn.metrics import accuracy_score\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=2)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.to(device)\n",
    "\n",
    "# Configure and apply LoRA\n",
    "config = LoraConfig(task_type=TaskType.SEQ_CLS,\n",
    "                    r=8, \n",
    "                    lora_alpha=16,\n",
    "                    lora_dropout=0.1,\n",
    "                    #target_modules=[\"c_attn\", \"c_proj\"]\n",
    "                   )\n",
    "lora_model = get_peft_model(model, config)\n",
    "lora_model.to(device)\n",
    "\n",
    "# Define compute metrics function\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {'accuracy': acc}\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    learning_rate=1e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791d3b9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2165' max='2165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2165/2165 05:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "trainer_test = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=dataset_dict['test'],\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "lora_model.eval()\n",
    "eval_results_before = trainer_test.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "673aa7c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluation results before training: {'eval_loss': 2.12790846824646, 'eval_accuracy': 0.49994225661161795, 'eval_runtime': 323.1704, 'eval_samples_per_second': 26.794, 'eval_steps_per_second': 6.699}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logger.info(f\"Evaluation results before training: {eval_results_before}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4feb5dc2",
   "metadata": {},
   "source": [
    "Note the initial test set eval_accuracy': 0.49994225661161795, which makes sense given that the model hasna't been trained on this task yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a2fd42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0f5da2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12988' max='12988' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12988/12988 1:24:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.683300</td>\n",
       "      <td>0.669151</td>\n",
       "      <td>0.596096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.632700</td>\n",
       "      <td>0.642985</td>\n",
       "      <td>0.640333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./finetuned/tokenizer_config.json',\n",
       " './finetuned/special_tokens_map.json',\n",
       " './finetuned/vocab.json',\n",
       " './finetuned/merges.txt',\n",
       " './finetuned/added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    learning_rate=1e-5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict['train'],\n",
    "    eval_dataset=dataset_dict['validation'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer)\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model and tokenizer\n",
    "lora_model.save_pretrained('./finetuned')\n",
    "tokenizer.save_pretrained('./finetuned')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "863ec66e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2165' max='2165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2165/2165 05:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluation after training: {'eval_loss': 0.6388337016105652, 'eval_accuracy': 0.6470724102090311, 'eval_runtime': 325.3416, 'eval_samples_per_second': 26.615, 'eval_steps_per_second': 6.655}\n"
     ]
    }
   ],
   "source": [
    "# After training\n",
    "from peft import AutoPeftModelForSequenceClassification\n",
    "inf_model = AutoPeftModelForSequenceClassification.from_pretrained(\"./finetuned\", num_labels=2)\n",
    "inf_model.config.pad_token_id = inf_model.config.eos_token_id\n",
    "\n",
    "trainer_test = Trainer(\n",
    "    model=inf_model,\n",
    "    args=training_args,\n",
    "    eval_dataset=dataset_dict['test'],\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "eval_results_after = trainer_test.evaluate()\n",
    "\n",
    "logger.info(f\"Evaluation after training: {eval_results_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ac35e5",
   "metadata": {},
   "source": [
    "Note that after training just 2 epochs, the test set performance on the test set has increased to eval_accuracy':  0.6470724102090311."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4021b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'The test is complete'\\prediction: __casual__\n",
      "Text: 'I want to hurt you'\\prediction: __casual__\n",
      "Text: 'Please hand me a pencil'\\prediction: __casual__\n",
      "Text: 'Please hand me a pencil so that I may kill myself'\\prediction: __needs_intervention__\n"
     ]
    }
   ],
   "source": [
    "def classify(text: str):\n",
    "    lookup = {0: '__casual__', 1: '__needs_intervention__'}\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inf_model.to(device)\n",
    "    ins = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = inf_model(**ins)\n",
    "        logits = outputs.logits\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    cls_id = probabilities.argmax().item()\n",
    "    pred_label = lookup[cls_id]\n",
    "    return pred_label\n",
    "\n",
    "# Examples\n",
    "text = \"The test is complete\"\n",
    "cls = classify(text)\n",
    "print(f\"Text: '{text}'\\prediction: {cls}\")\n",
    "\n",
    "text = \"I want to hurt you\"\n",
    "cls = classify(text)\n",
    "print(f\"Text: '{text}'\\prediction: {cls}\")\n",
    "\n",
    "text = \"Please hand me a pencil\"\n",
    "cls = classify(text)\n",
    "print(f\"Text: '{text}'\\prediction: {cls}\")\n",
    "\n",
    "text = \"Please hand me a pencil so that I may kill myself\"\n",
    "cls = classify(text)\n",
    "print(f\"Text: '{text}'\\prediction: {cls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7488b42e",
   "metadata": {},
   "source": [
    "The model appears to works somewhat, and is improved realtive to the base gpt-2 model. It could likely be trained more for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b356c456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a0b907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b57f01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca89d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654192ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b38d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39222365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571524a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c213886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8161f08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa839a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fc7112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a231d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4912d562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f334b29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a38f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
